<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>LIS500 Project 3 - Audio-Based Identity Classifier</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>

    <!-- Header with Navbar -->
    <header class="header">
      <h1>Audio Identity Classifier</h1>
      <nav>
        <ul class="menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main">

      <section class="introduction">
        <h2>Introduction</h2>
        <p>This project explores how machine learning algorithms classify voice-based identity attributes-specifically gender and ethnicity-from short audio samples using a Teachable Machine model.</p>
        <p>We created a browser-based classifier that attempts to distinguish voice features based on sound input, while also examining the limitations and potential risks of such technologies.</p>
        <p>This work is deeply informed by Joy Buolamwini's <em>Unmasking AI</em>, which questions the assumptions and embedded biases in AI systems, especially those that involve identity classification.</p>
        <iframe width="720" height="480" src="https://www.youtube.com/watch?v=l0mILNVQwSM"></iframe>
      </section>

      <img src="images/TeachableMachine.png" alt="Model Diagram" height="400" width="600" />

      <section class="content">
        <div class="contenttext">
          <h2>Reflections on <em>Unmasking AI</em></h2>
          <p>Buolamwini's writing was a powerful reminder that AI is not neutral. As we tested our model on diverse voice recordings, it became clear how much performance varied based on pitch, accent, and tone. This variation reflects a deeper issue: many models aren't built to serve all people equally.</p>
          <p>Her concept of the “coded gaze” helped us critique our own model and identify its blind spots—particularly around voice gendering and ethnic representation. This project is our way of participating in a more equitable AI discourse.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Project Statements</h2>
          <h3>Seung Min's</h3>
          <p>
            Working on this project was eye-opening in more ways than I expected. The project began as a fun exploration of training a voice classifier to recognize words like “hello,” “yes,” and “no.” But the deeper I got into testing, especially with non-native English speakers, the more it revealed the unsettling reality: even clear, correct speech could be consistently misunderstood by AI. And often, that misunderstanding came down to one thing - accent.
          </p>
          <p>
            The initial spark came from a TikTok trend where users had to pronounce words to guide a ball through a virtual hoop. I noticed that many Eastern creators - despite articulating the words correctly - were repeatedly failing the game. Their slight variations in tone, pitch, or rhythm led to misclassifications. At first glance, it looked like just another viral challenge. But it hinted at something deeper: a systemic pattern of exclusion. This casual frustration inspired me to take up this project, which explored the idea of how difficult of a concept accent bias rectification is and its implications.
          </p>
          <p>
            Joy Buolamwini’s <em>Unmasking AI</em> hit especially hard during this process. Her concept of the “coded gaze” - the idea that AI reflects the worldview of its creators - was no longer just theoretical. I saw it in action. Our model, trained on a subset of accents, occasionally failed to recognize the voices of my Korean, Mandarin, and Spanish speaking friends. Despite speaking the same words, their intonation or pitch would throw off the classifier. It didn’t “hear” them. And in that failure, it rendered them invisible.
          </p>
          <p>
            The image Buolamwini shares - having to wear a white mask to be recognized by a facial recognition system - stuck with me as I watched my friends repeat simple words, again and again, only to be misclassified or ignored. There was no "accent mask" they could put on to make the model listen. That failure wasn’t a fluke; it was a reflection of the model’s blind spots - and mine. I realized that AI doesn’t just exclude by accident. It excludes by design when diversity isn’t built into the training process.
          </p>
          <p>
            This experience forced me to confront how easily bias sneaks into our systems - not because of malicious intent but because of the assumptions we don’t question. I used English speakers/accents to train the classifier, thinking it was a neutral starting point. But it wasn’t. It was just one voice, with one accent, representing one version of English. Everyone else became an outlier.
          </p>
          <p>
            As an aspiring technologist, this was humbling. <em>Unmasking AI</em> taught me that fairness isn’t a checkbox - it’s a responsibility. Inclusion has to be intentional. I don’t want to build systems that only understand people who sound like me. I want to build systems that listen to everyone. This project - and Buolamwini’s work - taught me that ethical AI begins not with code but with care. With listening. With seeing who’s left out and choosing to bring them in.
          </p>




          <h3>Dui's</h3>
          <p>
            Reflecting on Joy Buolamwini’s <em>Unmasking AI</em> and our project work, I was struck by how theory and practice can deeply intersect. At first, I focused on technical performance—labeling, clean audio, balanced data. But as I saw the model’s accuracy shift depending on the speaker, I realized this was about more than data quality—it was about representation and power.
          </p>
          <p>
            Though I’m bilingual in Mandarin and English, the model still recognized my voice best. That made me wonder if I was unintentionally designing it to understand people like me, while excluding others.
          </p>
          <p>
            <em>Unmasking AI</em> helped me see that bias often hides in subtle design choices—whose voices are recorded more, which samples are deemed “good.” It challenged me to think beyond technical success and consider who my systems serve—and who they ignore.
          </p>
          <p>
            This experience shifted my thinking. Fairness in AI isn’t just technical—it’s cultural, personal, and systemic. It pushed me to ask deeper questions: who gets to be heard, and who gets left out?
          </p>

        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Ethical Implications</h2>
          <p>Training models to classify human attributes—like gender and ethnicity—raises significant ethical questions. What happens when such models are deployed at scale? Are they ever truly accurate or fair?</p>
          <p>Our classifier is deliberately simple, but it demonstrates how data bias, lack of diversity, and limited testing can amplify existing social inequalities. We hope users will view this as a prompt for reflection, not just a tool.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>What Was Learned</h2>
          <p>This project taught us that creating technology is not just a technical act—it's a political one. Every design choice reflects values, and every model embeds assumptions about identity. Following Buolamwini's message, we now ask better questions: Who is this for? Who does it fail?</p>
          <p>Machine learning should not be about classification alone—it should be about care, transparency, and responsibility. Our hope is that future technologists will take those ideas seriously, even when the stakes seem small.</p>
        </div>
      </section>


      <div class="button">
        <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">Learn More</a>
      </div>

    </main>

    <!-- Footer -->
    <footer class="footer">
      <p>&copy; 2025 Dui Cao & Seung-Min Yu</p>
    </footer>

  </body>
</html>
