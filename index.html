<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>LIS500 Project 3 - Audio-Based Identity Classifier</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>

    <!-- Header with Navbar -->
    <header class="header">
      <h1>Audio Identity Classifier</h1>
      <nav>
        <ul class="menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main">

      <section class="introduction">
        <h2>Introduction</h2>
        <p>This project explores how machine learning algorithms classify voice-based identity attributes—specifically gender and ethnicity—from short audio samples using a Teachable Machine model.</p>
        <p>We created a browser-based classifier that attempts to distinguish voice features based on sound input, while also examining the limitations and potential risks of such technologies.</p>
        <p>This work is deeply informed by Joy Buolamwini's <em>Unmasking AI</em>, which questions the assumptions and embedded biases in AI systems, especially those that involve identity classification.</p>
        <iframe width="720" height="480" src="https://youtube.com/embed/Dk5YDQvtR_M?si=LSkaFppyp-kh5tm4" allowfullscreen></iframe>
      </section>

      <img src="images/TeachableMachine.png" alt="Model Diagram" height="400" width="600" />

      <section class="content">
        <div class="contenttext">
          <h2>Reflections on <em>Unmasking AI</em></h2>
          <p>Buolamwini's writing was a powerful reminder that AI is not neutral. As we tested our model on diverse voice recordings, it became clear how much performance varied based on pitch, accent, and tone. This variation reflects a deeper issue: many models aren't built to serve all people equally.</p>
          <p>Her concept of the “coded gaze” helped us critique our own model and identify its blind spots—particularly around voice gendering and ethnic representation. This project is our way of participating in a more equitable AI discourse.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Reflection: Lessons from <em>Unmasking AI</em></h2>
          <h3>Dui's Reflection</h3>
          <p>
            Reflecting on Joy Buolamwini’s Unmasking AI and the work we did in this project, I was struck by how closely theory and practice can intersect. While training the model, I initially focused on ensuring good technical performance—clear labeling, consistent audio, and a balanced dataset. But as I observed the model's accuracy fluctuate depending on who was speaking, I realized that something deeper was happening. This wasn’t just about “data quality”—it was about representation and power.
          </p>
          <p>
            Buolamwini's term “coded gaze” became especially meaningful. As she describes, the gaze of the algorithm is shaped by the perspectives and priorities of its creators. I had assumed my bilingualism (speaking both Mandarin and English) would help me be more inclusive. But I still noticed the model recognized my voice more accurately than others, which made me pause. I began to wonder: Was I unconsciously training the model to understand people like me, while sidelining others?
          </p>
          <p>
            Reading Unmasking AI helped me recognize that exclusion in tech isn’t always loud or intentional. It’s often silent, embedded in choices we don't think twice about—like whose voice gets recorded more times or which sample we consider “good enough.” That’s why Buolamwini’s argument that bias is built into the design process hit home. It made me reconsider what it means to be a responsible developer or designer. It’s not just about “making things work”—it’s about who they work for, and who they fail.
          </p>
          <p>
            Reading <em>Unmasking AI</em> made me critically evaluate what kinds of "voices" get heard and respected in technological systems. It became clear that this wasn’t just a technical issue—it was personal, cultural, and systemic. This experience encouraged me to think beyond accuracy and instead ask: who gets to participate in this system, and who gets left out?
          </p>

          <h3>Seung min's Reflection</h3>
          <p>
            Joy Buolamwini’s Unmasking AI also resonated with me on a personal level, especially her discussion about the relationship between data, identity, and invisibility. I experienced this first-hand during our testing phase. Despite speaking clearly and consistently, the model often misclassified or failed to recognize my voice. This wasn’t a random glitch—it was a symptom of underrepresentation. My accent wasn’t captured as thoroughly in the training data, and the result was exclusion.
          </p>
          <p>
            This made me realize how AI reflects not just technology, but human choices. When Buolamwini speaks about the structural invisibility that marginalized groups face, I understood exactly what she meant. If our voices are not included in the training process, we become invisible to the machine. And that invisibility is not neutral—it has real consequences, especially as voice interfaces become more embedded in daily life, from customer service bots to home assistants.
          </p>
          <p>
            What stood out most in Unmasking AI is that this is not just a problem for people “out there”—it can affect anyone whose identity doesn’t match the assumed “default” of tech developers. As an aspiring technologist, I took this project as a lesson in humility. Bias doesn’t just come from bad actors—it comes from well-intentioned people who fail to look critically at their assumptions. Moving forward, I want to ensure I never build a system that only works for people like me, but one that listens to everyone.
          </p>
          <p>
            This reminded me that exclusion doesn’t only happen through malicious intent but also through unintentional oversight. By not including more diverse voice samples, we repeated the exact kinds of bias Joy describes. As a student and an aspiring technologist, this was a wake-up call: that fairness in AI starts with awareness, responsibility, and proactive inclusion.
          </p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Ethical Implications</h2>
          <p>Training models to classify human attributes—like gender and ethnicity—raises significant ethical questions. What happens when such models are deployed at scale? Are they ever truly accurate or fair?</p>
          <p>Our classifier is deliberately simple, but it demonstrates how data bias, lack of diversity, and limited testing can amplify existing social inequalities. We hope users will view this as a prompt for reflection, not just a tool.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>What Was Learned</h2>
          <p>This project taught us that creating technology is not just a technical act—it's a political one. Every design choice reflects values, and every model embeds assumptions about identity. Following Buolamwini's message, we now ask better questions: Who is this for? Who does it fail?</p>
          <p>Machine learning should not be about classification alone—it should be about care, transparency, and responsibility. Our hope is that future technologists will take those ideas seriously, even when the stakes seem small.</p>
        </div>
      </section>


      <div class="button">
        <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">Learn More</a>
      </div>

    </main>

    <!-- Footer -->
    <footer class="footer">
      <p>&copy; 2025 Dui Cao & Seung-Min Yu</p>
    </footer>

  </body>
</html>
