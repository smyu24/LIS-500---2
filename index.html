<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>LIS500 Project 3 - Audio-Based Identity Classifier</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>

    <!-- Header with Navbar -->
    <header class="header">
      <h1>Audio Identity Classifier</h1>
      <nav>
        <ul class="menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main">

      <section class="introduction">
        <h2>Introduction</h2>
        <p>This project explores how machine learning algorithms classify voice-based identity attributes—specifically gender and ethnicity—from short audio samples using a Teachable Machine model.</p>
        <p>We created a browser-based classifier that attempts to distinguish voice features based on sound input, while also examining the limitations and potential risks of such technologies.</p>
        <p>This work is deeply informed by Joy Buolamwini's <em>Unmasking AI</em>, which questions the assumptions and embedded biases in AI systems, especially those that involve identity classification.</p>
        <iframe width="720" height="480" src="" allowfullscreen></iframe>
      </section>

      <img src="images/TeachableMachine.png" alt="Model Diagram" height="400" width="600" />

      <section class="content">
        <div class="contenttext">
          <h2>Reflections on <em>Unmasking AI</em></h2>
          <p>Buolamwini's writing was a powerful reminder that AI is not neutral. As we tested our model on diverse voice recordings, it became clear how much performance varied based on pitch, accent, and tone. This variation reflects a deeper issue: many models aren't built to serve all people equally.</p>
          <p>Her concept of the “coded gaze” helped us critique our own model and identify its blind spots—particularly around voice gendering and ethnic representation. This project is our way of participating in a more equitable AI discourse.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Project Statements</h2>
          <h3>Seung Min's</h3>
          <p>
            Working on <em>Invisible Voice 2.0 - The Accent Bias Experiment</em> was eye-opening in more ways than I expected. The project began as a fun exploration - training a voice classifier to recognize words like “hello,” “yes,” and “no.” But the deeper I got into testing, especially with non-native English speakers, the more it revealed the unsettling reality: even clear, correct speech could be consistently misunderstood by AI. And often, that misunderstanding came down to one thing - accent.
          </p>
          <p>
            Joy Buolamwini’s <em>Unmasking AI</em> hit especially hard during this process. Her concept of the “coded gaze” - the idea that AI reflects the worldview of its creators - was no longer just theoretical. I saw it in action. The model, trained mostly on my voice, failed to recognize the voices of my Korean, Indian, and Spanish-speaking friends. Despite speaking the same words, their intonation or pitch would throw off the classifier. It didn’t “hear” them. And in that failure, it rendered them invisible.
          </p>
          <p>
            The image Buolamwini shares - having to wear a white mask to be recognized by a facial recognition system - stuck with me as I watched my friends repeat simple words, again and again, only to be misclassified or ignored. There was no "accent mask" they could put on to make the model listen. That failure wasn’t a fluke; it was a reflection of the model’s blind spots - and mine. I realized that AI doesn’t just exclude by accident. It excludes by design, when diversity isn’t built into the training process.
          </p>
          <p>
            This experience forced me to confront how easily bias sneaks into our systems - not because of malicious intent, but because of the assumptions we don’t question. I used my voice to train the classifier, thinking it was a neutral starting point. But it wasn’t. It was just one voice, with one accent, representing one version of English. Everyone else became an outlier.
          </p>
          <p>
            As an aspiring technologist, this was humbling. <em>Unmasking AI</em> taught me that fairness isn’t a checkbox - it’s a responsibility. Inclusion has to be intentional. I don’t want to build systems that only understand people who sound like me. I want to build systems that listen to everyone. This project - and Buolamwini’s work - taught me that ethical AI begins not with code, but with care. With listening. With seeing who’s left out, and choosing to bring them in.
          </p>



          <h3>Dui's Reflection</h3>
          <p>
            Reflecting on Joy Buolamwini’s Unmasking AI and the work we did in this project, I was struck by how closely theory and practice can intersect. While training the model, I initially focused on ensuring good technical performance—clear labeling, consistent audio, and a balanced dataset. But as I observed the model's accuracy fluctuate depending on who was speaking, I realized that something deeper was happening. This wasn’t just about “data quality”—it was about representation and power.
          </p>
          <p>
            Buolamwini's term “coded gaze” became especially meaningful. As she describes, the gaze of the algorithm is shaped by the perspectives and priorities of its creators. I had assumed my bilingualism (speaking both Mandarin and English) would help me be more inclusive. But I still noticed the model recognized my voice more accurately than others, which made me pause. I began to wonder: Was I unconsciously training the model to understand people like me, while sidelining others?
          </p>
          <p>
            Reading Unmasking AI helped me recognize that exclusion in tech isn’t always loud or intentional. It’s often silent, embedded in choices we don't think twice about—like whose voice gets recorded more times or which sample we consider “good enough.” That’s why Buolamwini’s argument that bias is built into the design process hit home. It made me reconsider what it means to be a responsible developer or designer. It’s not just about “making things work”—it’s about who they work for, and who they fail.
          </p>
          <p>
            Reading <em>Unmasking AI</em> made me critically evaluate what kinds of "voices" get heard and respected in technological systems. It became clear that this wasn’t just a technical issue—it was personal, cultural, and systemic. This experience encouraged me to think beyond accuracy and instead ask: who gets to participate in this system, and who gets left out?
          </p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Ethical Implications</h2>
          <p>Training models to classify human attributes—like gender and ethnicity—raises significant ethical questions. What happens when such models are deployed at scale? Are they ever truly accurate or fair?</p>
          <p>Our classifier is deliberately simple, but it demonstrates how data bias, lack of diversity, and limited testing can amplify existing social inequalities. We hope users will view this as a prompt for reflection, not just a tool.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>What Was Learned</h2>
          <p>This project taught us that creating technology is not just a technical act—it's a political one. Every design choice reflects values, and every model embeds assumptions about identity. Following Buolamwini's message, we now ask better questions: Who is this for? Who does it fail?</p>
          <p>Machine learning should not be about classification alone—it should be about care, transparency, and responsibility. Our hope is that future technologists will take those ideas seriously, even when the stakes seem small.</p>
        </div>
      </section>


      <div class="button">
        <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">Learn More</a>
      </div>

    </main>

    <!-- Footer -->
    <footer class="footer">
      <p>&copy; 2025 Dui Cao & Seung-Min Yu</p>
    </footer>

  </body>
</html>
