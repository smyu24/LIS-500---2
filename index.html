<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>LIS500 Project 3 - Audio-Based Identity Classifier</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>

    <!-- Header with Navbar -->
    <header class="header">
      <h1>Audio Identity Classifier</h1>
      <nav>
        <ul class="menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main">

      <section class="introduction">
        <h2>Introduction</h2>
        <p>This project explores how machine learning algorithms classify voice-based identity attributes—specifically gender and ethnicity—from short audio samples using a Teachable Machine model.</p>
        <p>We created a browser-based classifier that attempts to distinguish voice features based on sound input, while also examining the limitations and potential risks of such technologies.</p>
        <p>This work is deeply informed by Joy Buolamwini's <em>Unmasking AI</em>, which questions the assumptions and embedded biases in AI systems, especially those that involve identity classification.</p>
        <iframe width="720" height="480" src="https://youtube.com/embed/Dk5YDQvtR_M?si=LSkaFppyp-kh5tm4" allowfullscreen></iframe>
      </section>

      <img src="images/catdogmodel.png" alt="Model Diagram" height="400" width="600" />

      <section class="content">
        <div class="contenttext">
          <h2>Reflections on <em>Unmasking AI</em></h2>
          <p>Buolamwini's writing was a powerful reminder that AI is not neutral. As we tested our model on diverse voice recordings, it became clear how much performance varied based on pitch, accent, and tone. This variation reflects a deeper issue: many models aren't built to serve all people equally.</p>
          <p>Her concept of the “coded gaze” helped us critique our own model and identify its blind spots—particularly around voice gendering and ethnic representation. This project is our way of participating in a more equitable AI discourse.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Ethical Implications</h2>
          <p>Training models to classify human attributes—like gender and ethnicity—raises significant ethical questions. What happens when such models are deployed at scale? Are they ever truly accurate or fair?</p>
          <p>Our classifier is deliberately simple, but it demonstrates how data bias, lack of diversity, and limited testing can amplify existing social inequalities. We hope users will view this as a prompt for reflection, not just a tool.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>What Was Learned</h2>
          <p>This project taught us that creating technology is not just a technical act—it's a political one. Every design choice reflects values, and every model embeds assumptions about identity. Following Buolamwini's message, we now ask better questions: Who is this for? Who does it fail?</p>
          <p>Machine learning should not be about classification alone—it should be about care, transparency, and responsibility. Our hope is that future technologists will take those ideas seriously, even when the stakes seem small.</p>
        </div>
      </section>

      <div class="button">
        <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">Learn More</a>
      </div>

    </main>

    <!-- Footer -->
    <footer class="footer">
      <p>&copy; 2025 Dui Cao & Seung-Min Yu. All rights reserved.</p>
    </footer>

  </body>
</html>
