<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>LIS500 Project 3 - Audio-Based Identity Classifier</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>

    <!-- Header with Navbar -->
    <header class="header">
      <h1>Audio Identity Classifier</h1>
      <nav>
        <ul class="menu">
          <li><a href="index.html">Home</a></li>
          <li><a href="machine.html">Our Machine</a></li>
        </ul>
      </nav>
    </header>

    <!-- Main Content -->
    <main class="main">

      <section class="introduction">
        <h2>Introduction</h2>
        <p>This project explores how machine learning algorithms classify voice-based identity attributes—specifically gender and ethnicity—from short audio samples using a Teachable Machine model. Inspired by Joy Buolamwini’s <em>Unmasking AI</em> (2023), we investigated what she calls the <strong>coded gaze</strong>: the way AI systems reflect the biases of their creators rather than objective truths (p. 18). Just as Buolamwini discovered facial recognition tools failed to see her unless she wore a white mask, we found our voice model also excluded users with certain accents or voice tones. Our project asks: <em>Who gets heard? Who gets erased?</em></p>
        <p>We created a browser-based classifier that attempts to distinguish voice features based on sound input, while also examining the limitations and potential risks of such technologies.</p>
        <p>This work is deeply informed by Joy Buolamwini's <em>Unmasking AI</em>, which questions the assumptions and embedded biases in AI systems, especially those that involve identity classification.</p>
        <iframe width="720" height="480" src="https://www.youtube.com/embed/l0mILNVQwSM?si=yf1CrTbDUVUNgDlx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      </section>

      <img src="images/TeachableMachine.png" alt="Model Diagram" height="400" width="600" />

      <section class="content">
        <div class="contenttext">
          <h2>Reflections on <em>Unmasking AI</em></h2>
          <p>Buolamwini's writing was a powerful reminder that AI is not neutral. As we tested our model on diverse voice recordings, it became clear how much performance varied based on pitch, accent, and tone. This variation reflects a deeper issue: many models aren't built to serve all people equally.</p>
          <p>Her concept of the “coded gaze” helped us critique our own model and identify its blind spots—particularly around voice gendering and ethnic representation. This project is our way of participating in a more equitable AI discourse.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Project Statements</h2>
          <h3>Seung Min's</h3>
          <p>
            Working on this project was eye-opening in more ways than I expected. The project began as a fun exploration of training a voice classifier to recognize words like “hello,” “yes,” and “no.” But the deeper I got into testing, especially with non-native English speakers, the more it revealed the unsettling reality: even clear, correct speech could be consistently misunderstood by AI. And often, that misunderstanding came down to one thing - accent.
          </p>
          <p>
            The initial spark came from a TikTok trend where users had to pronounce words to guide a ball through a virtual hoop. I noticed that many Eastern creators - despite articulating the words correctly - were repeatedly failing the game. Their slight variations in tone, pitch, or rhythm led to misclassifications. At first glance, it looked like just another viral challenge. But it hinted at something deeper: a systemic pattern of exclusion. This casual frustration inspired me to take up this project, which explored the idea of how difficult of a concept accent bias rectification is and its implications.
          </p>
          <p>
            Joy Buolamwini’s <em>Unmasking AI</em> hit especially hard during this process. Her concept of the “coded gaze” - the idea that AI reflects the worldview of its creators - was no longer just theoretical. I saw it in action. Our model, trained on a subset of accents, occasionally failed to recognize the voices of my Korean, Mandarin, and Spanish speaking friends. Despite speaking the same words, their intonation or pitch would throw off the classifier. It didn’t “hear” them. And in that failure, it rendered them invisible.
          </p>
          <p>
            The image Buolamwini shares - having to wear a white mask to be recognized by a facial recognition system - stuck with me as I watched my friends repeat simple words, again and again, only to be misclassified or ignored. There was no "accent mask" they could put on to make the model listen. That failure wasn’t a fluke; it was a reflection of the model’s blind spots - and mine. I realized that AI doesn’t just exclude by accident. It excludes by design when diversity isn’t built into the training process.
          </p>
          <p>
            This experience forced me to confront how easily bias sneaks into our systems - not because of malicious intent but because of the assumptions we don’t question. I used English speakers/accents to train the classifier, thinking it was a neutral starting point. But it wasn’t. It was just one voice, with one accent, representing one version of English. Everyone else became an outlier.
          </p>
          <p>
            As an aspiring technologist, this was humbling. <em>Unmasking AI</em> taught me that fairness isn’t a checkbox - it’s a responsibility. Inclusion has to be intentional. I don’t want to build systems that only understand people who sound like me. I want to build systems that listen to everyone. This project - and Buolamwini’s work - taught me that ethical AI begins not with code but with care. With listening. With seeing who’s left out and choosing to bring them in.
          </p>




          <h3>Dui's</h3>
          <p>
            Reflecting on Joy Buolamwini’s <em>Unmasking AI</em> and our project work, I was struck by how theory and practice can deeply intersect. At first, I focused on technical performance—labeling, clean audio, balanced data. But as I saw the model’s accuracy shift depending on the speaker, I realized this was about more than data quality—it was about representation and power.
          </p>
          <p>
            Though I’m bilingual in Mandarin and English, the model still recognized my voice best. That made me wonder if I was unintentionally designing it to understand people like me, while excluding others.
          </p>
          <p>
            <em>Unmasking AI</em> helped me see that bias often hides in subtle design choices—whose voices are recorded more, which samples are deemed “good.” It challenged me to think beyond technical success and consider who my systems serve—and who they ignore.
          </p>
          <p>
            This experience shifted my thinking. Fairness in AI isn’t just technical—it’s cultural, personal, and systemic. It pushed me to ask deeper questions: who gets to be heard, and who gets left out?
          </p>

        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>Ethical Implications</h2>
          <p>Training models to classify human attributes—like gender and ethnicity—raises significant ethical questions. What happens when such models are deployed at scale? Are they ever truly accurate or fair?</p>
          <p>Our classifier is deliberately simple, but it demonstrates how data bias, lack of diversity, and limited testing can amplify existing social inequalities. We hope users will view this as a prompt for reflection, not just a tool.</p>
          <p>As Buolamwini emphasizes, “We cannot have racial justice if we adopt technical tools... that only further incarcerate communities of color” (p. 33). Our classifier is a small, browser-based experiment, but the design choices we make—even about what accents to include—reflect assumptions that scale. This isn’t just about voice—it’s about whose identity gets reduced, misread, or denied by technology.</p>
        </div>
      </section>

      <section class="content">
        <div class="contenttext">
          <h2>What Was Learned</h2>
          <p>This project taught us that designing technology is never neutral—it reflects values. Following Buolamwini’s conclusion on page 34, we now see that “not building a tool or not collecting intrusive data is an option... and one that should be the first consideration.” The goal isn’t just functional models, but inclusive, ethical ones. We must ask not just <em>can</em> we build, but <em>should</em> we?</p>            
          <p>Machine learning should not be about classification alone—it should be about care, transparency, and responsibility. Our hope is that future technologists will take those ideas seriously, even when the stakes seem small.</p>
          </div>
      </section>


      <div class="button">
        <a href="https://thecodingtrain.com/tracks/teachable-machine/teachable-machine/3-sound-classification" target="_blank">Learn More About Teachable Machines</a>
      </div>

      <section class="content">
        <div class="contenttext">
          <h2>Connecting to the Implicit Bias Resource Guide</h2>
          <p>Our work on the Audio Identity Classifier does not exist in a vacuum. It builds upon and intersects with our broader exploration of how unconscious attitudes shape systems, policies, and technologies—work that is detailed in our companion website: the <a href="your-link-to-guide.html" target="_blank">Implicit Bias Resource Guide</a>.</p>
          <p>This guide dives into how implicit bias—unconscious stereotypes that affect perception and decision-making—manifests in key areas like education, hiring, healthcare, and criminal justice. While the classifier explores bias in machine learning systems, the resource guide maps the real-world consequences of such biases across society.</p>
          <p>Both projects are united by a shared goal: to expose and challenge the hidden forces that marginalize voices and identities. Whether it’s a model misclassifying a voice due to accent, or a hiring algorithm favoring certain names, these patterns stem from the same root—unexamined bias embedded in data, culture, and design.</p>
          <p>As we continue to explore and question technology’s role in shaping identity, we invite you to visit the <a href="your-link-to-guide.html" target="_blank">Implicit Bias Resource Guide</a> to deepen your understanding of these systemic dynamics.</p>
        </div>
      </section>

      <div class="button">
        <a href="https://smyu24.github.io/LIS-500/resources.html" target="_blank">Learn More about Implicit Bias in Project Two IBT Guide</a>
      </div>

    </main>

    <!-- Footer -->
    <footer class="footer">
      <p>&copy; 2025 Dui Cao & Seung-Min Yu</p>
    </footer>

  </body>
</html>
