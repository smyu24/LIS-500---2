<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Invisible Voice 2.0 – Accent, Gender & Ethnicity Bias Classifier</title>
  
</head>
<body>
  <header>
    <h1>Invisible Voice 2.0</h1>
    <p>A Teachable Machine Experiment on Gender & Ethnic Bias in Voice Recognition</p>
  </header>

  <div class="content">
    <div class="highlight">
      <strong>Try it out:</strong> Speak phrases into your mic and see how the model classifies your voice in terms of perceived gender or ethnicity.
    </div>

    <iframe src="https://teachablemachine.withgoogle.com/models/YOUR_MODEL_URL/" allow="microphone; camera"></iframe>

    <h2>Project Statement: About Invisible Voice 2.0</h2>
    <p>
      Invisible Voice 2.0 is a critical AI and media literacy project exploring how machine learning models classify perceived gender and ethnicity from vocal audio input. Inspired by Joy Buolamwini’s groundbreaking book <em>Unmasking AI</em>, our team wanted to test the boundaries of fairness and inclusivity in systems that interpret human voice.
    </p>
    <p>
      Using <a href="https://teachablemachine.withgoogle.com/" target="_blank">Teachable Machine</a> by Google, we trained an audio classification model to categorize voices into broad gender and ethnic categories: for example, "Male - Indian", "Female - American", or "Unknown". Our dataset included recordings from friends, classmates, and public datasets that aimed to reflect a range of accents, tones, and speaking styles. The goal was not to reinforce stereotypes, but rather to critically examine whether machine learning models can (or should) attempt to infer identity from sound alone.
    </p>

    <h2>Reflections from <em>Unmasking AI</em></h2>
    <h3>Dui's Reflection</h3>
    <p>
      One of the key lessons I drew from Buolamwini’s work is her emphasis on the "coded gaze"—the idea that algorithms reflect the worldview and biases of those who build them. As I trained our model, I realized that even the decision of which categories to include is a subjective one. By framing voices through the lens of gender and ethnicity, we are imposing a system that may not reflect how individuals self-identify. This discomfort is productive: it reveals how flawed data collection and labeling processes can perpetuate misrepresentation.
    </p>
    <p>
      Reading <em>Unmasking AI</em> encouraged me to think deeply about data ethics. Who gets to define the labels? Who gets represented in the training set? Our model struggled to correctly classify more ambiguous or non-binary voices. This isn't just a technical limitation—it's a political one. The lack of flexibility in classification reflects broader systemic exclusion in AI systems.
    </p>

    <h3>Seung's Reflection</h3>
    <p>
      While building our classifier, I felt a constant tension between the simplicity of machine learning tools and the complexity of human identity. <em>Unmasking AI</em> made me realize that technology’s default mode is often exclusionary, not inclusive. For example, our model often labeled my voice as "Male - American" even though I speak English with a Korean accent. This misclassification highlighted not only the model's limitations but also the assumptions baked into how it was trained.
    </p>
    <p>
      Joy Buolamwini’s insistence on algorithmic accountability reminded me that we, as student developers, carry responsibility. It is not enough for the model to function—it must be transparent, fair, and inclusive. This project showed me that diversity in the training data is necessary but not sufficient. Inclusion must be a design value from the start.
    </p>

    <h2>What We Learned</h2>
    <p>
      We learned that algorithmic identity recognition is fraught with risk. While it might be technically possible to make guesses about gender and ethnicity from voice, the ethical implications are enormous. These classifications can reinforce stereotypes, make harmful assumptions, and exclude individuals who don’t conform to dominant norms. We also saw firsthand how models often fail with voices that are soft-spoken, accented, or don’t match the data majority.
    </p>
    <p>
      Our biggest takeaway is this: algorithmic design is not neutral. Every decision—how data is labeled, who is included, what categories exist—has moral weight. Reading Buolamwini’s work gave us a framework to analyze these issues critically and to treat machine learning not as a purely technical pursuit, but as a social and ethical one.
    </p>

    <h2>Explore the Code</h2>
    <p>
      View the source code and data documentation on our GitHub repository: <a href="https://github.com/YOUR_USERNAME/your-repo-name" target="_blank">GitHub Repository</a>
    </p>

    <h2>Video Demo</h2>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/YOUR_VIDEO_ID_HERE" frameborder="0" allowfullscreen></iframe>

    <h2>Project Team</h2>
    <ul>
      <li>Dui Cao</li>
      <li>Seung-Min Yu</li>
    </ul>
  </div>

  <footer>
    <p>© 2025 Invisible Voice 2.0 – Created in critical response to <em>Unmasking AI</em> by Joy Buolamwini.</p>
  </footer>

  <script>
    console.log("Voice bias classifier loaded. Speak into the mic to test.");
  </script>
</body>
</html>
