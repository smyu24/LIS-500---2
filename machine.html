<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <title>Interjection Classifier</title>
  <link rel="stylesheet" href="style.css" />
  
  <!-- p5.js + ml5.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/addons/p5.dom.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/addons/p5.sound.min.js"></script>
  <script src="https://unpkg.com/ml5@0.4.2/dist/ml5.min.js"></script>
</head>
<body>

  <!-- Header/Navbar -->
  <header class="header">
    <h1>Accent & Gender Classifier</h1>
    <nav>
      <ul class="menu">
        <li><a href="index.html">Home</a></li>
        <li><a href="machine.html">Our Machine</a></li>
      </ul>
    </nav>
  </header>

  <!-- Main Content -->
  <main class="main">

    <section class="introduction">
      <h2>Welcome to Our Project</h2>
      <p>
        This project focuses on detecting <strong>Accents + Gender</strong> of users in real time using a sound classifier. Creating this model has been an interesting ethical dilemma, and it was especially rewarding to see the accuracy post model tuning
      </p>
    </section>

    <section class="content">
      <h2>How It Works</h2>
      <p>
        We used <a href="https://teachablemachine.withgoogle.com/" target="_blank">Teachable Machine</a> by Google to train a sound classifier.
        It listens to ~30-second audio clips and compares them to the trained patterns based on pitch, rhythm, and tone.
      </p>
      <p>
        The model was trained with several common accents (total of 8 male & female), and it updates predictions live as you speak.
      </p>
    </section>

    <section class="content">
      <h2>Project Goals</h2>
      <ul class="contenttext">
        <li>Showcase how machine learning can interpret spontaneous human speech patterns.</li>
        <li>Explore emotional nuance in voice and sound.</li>
        <li>Utilize accessible tools like Teachable Machine and p5.js.</li>
        <li>Build a fun, interactive web-based demo.</li>
      </ul>
      <h2>Challenges & Learnings</h2>
      <p>
        Interjections are often subtle and vary based on tone and emotion. This made classification tricky—"hmm" could be thoughtful or confused depending on how it’s said.
      </p>
      <p>
        We learned that real-world audio is messy, and creating a responsive model requires a diverse dataset with clear labeling and consistency across voice types.
      </p>
    </section>

    <section class="content">
      <h2>Next Steps</h2>
      <ul class="contenttext">
        <li>Train with more interjection types and real-world background noise.</li>
        <li>Add support for multilingual interjections.</li>
        <li>Combine with emotion recognition models to provide context.</li>
        <li>Test use cases in education, accessibility, and human-computer interaction.</li>
      </ul>
    </section>

    <section class="content">
        <div class="contenttext">
          <h2>Explore the Code</h2>
          <p>
            View the source code and dataset information on our GitHub repository: <a href="https://github.com/YOUR_USERNAME/your-repo-name" target="_blank">GitHub Repository</a>
          </p>
        </div>
      </section>


    <section class="introduction">
      <h2>Try It Out!</h2>
      <p>
        Speak an interjection into your microphone (e.g., “wow,” “hmm,” “yay”) and our classifier will attempt to recognize it instantly.
      </p>
      <script src="model.js"></script>
    </section>
  </main>

</body>
</html>
